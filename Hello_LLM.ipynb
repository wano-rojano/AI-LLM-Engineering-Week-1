{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzZAscou7sWEghevnGK5xX5ywY0z9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Certainly! LangChain and LlamaIndex (formerly known as GPT Index) are both influential tools in the ecosystem of large language models (LLMs), but they serve distinct purposes and have different focuses. Here's a breakdown of their primary differences:\\n\\n**1. Purpose and Core Functionality**\\n\\n- **LangChain:**\\n  - **Primary Focus:** Building complex, multi-step LLM applications and pipelines.\\n  - **Functionality:** Provides abstractions and frameworks for chaining together prompts, models, memory, and external tools (like APIs, databases). It facilitates creating conversational agents, question-answering systems, and workflows that require orchestration of multiple components.\\n  - **Use Case:** Orchestrating LLM workflows, managing conversation state, integrating with plugins and external data sources.\\n\\n- **LlamaIndex (GPT Index):**\\n  - **Primary Focus:** Facilitating the ingestion, indexing, and querying of large external data sources using LLMs.\\n  - **Functionality:** Simplifies building semantic search systems over documents, databases, or other knowledge bases. It creates vector or hybrid indices that enable efficient retrieval and question answering over custom datasets.\\n  - **Use Case:** Building semantic search engines, chatbots that answer questions based on custom data, knowledge management.\\n\\n**2. Scope and Application Domains**\\n\\n- **LangChain:**\\n  - Suitable for developing end-to-end applications involving LLMs, such as chatbots, virtual assistants, or multi-step data processing pipelines.\\n  - Offers integrations with many models, tools, and memory mechanisms to maintain context over interactions.\\n\\n- **LlamaIndex:**\\n  - More specialized in managing document-driven applications—loading, preprocessing, indexing, and querying large document collections.\\n  - Ideal for organizations needing to embed domain-specific knowledge into LLM-based retrieval systems.\\n\\n**3. Technical Approach**\\n\\n- **LangChain:**\\n  - Emphasizes modularity with chains, agents, tools, and memory to enable complex, customizable workflows.\\n  - Examples include chatbots that can invoke external APIs or perform multi-turn reasoning.\\n\\n- **LlamaIndex:**\\n  - Focuses on data ingestion and index building. Supports various indexing strategies like vector, keyword, or hybrid.\\n  - Underpins applications requiring retrieval augmented generation (RAG) techniques.\\n\\n**4. Integration and Extensibility**\\n\\n- **LangChain:**\\n  - Offers extensive integrations with different LLM providers (OpenAI, Hugging Face, etc.), data sources, and tools.\\n  - Flexible architecture to build bespoke workflows.\\n\\n- **LlamaIndex:**\\n  - Provides connectors for different data sources (local files, cloud storage, APIs) to create indices.\\n  - Designed to seamlessly integrate with LLMs for retrieval and question answering.\\n\\n---\\n\\n### Summary Table\\n\\n| Aspect                     | **LangChain**                               | **LlamaIndex**                              |\\n|----------------------------|----------------------------------------------|--------------------------------------------|\\n| Main Purpose               | Build multi-step LLM applications and workflows | Indexing and querying large datasets with LLMs |\\n| Focus Area                 | Orchestration, chains, agents, memory       | Data ingestion, indexing, retrieval       |\\n| Typical Use Cases          | Chatbots, automation, complex workflows     | Semantic search, document Q&A            |\\n| Data Handling              | External tools, APIs, memory, prompts        | Large datasets, documents, knowledge bases |\\n| Extensibility              | Highly modular, supports plugins and integrations | Supports various indexing strategies      |\\n\\n---\\n\\n**In essence:**  \\n- Use **LangChain** if you're building applications that require complex interactions, memory management, and multi-step reasoning workflows involving LLMs.  \\n- Use **LlamaIndex** when your goal is to organize large collections of data and build fast, accurate retrieval systems that leverage LLMs for answering questions based on those datasets.\\n\\nLet me know if you'd like more detailed comparisons or use-case examples!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754012918, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=786, prompt_tokens=19, total_tokens=805, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building language-based applications, but they serve different purposes and have distinct features. Here's a high-level comparison:\n",
              "\n",
              "**1. Purpose and Focus**\n",
              "\n",
              "- **LangChain**:  \n",
              "  - Focuses on building end-to-end conversational AI and large language model (LLM) applications.  \n",
              "  - Provides tools for chaining multiple prompts, managing conversations, integrations with various language models, and orchestrating complex workflows.\n",
              "\n",
              "- **LlamaIndex (GPT Index)**:  \n",
              "  - Primarily designed for creating efficient retrieval-augmented generation (RAG) systems.  \n",
              "  - Focuses on indexing large unstructured data sources (like documents, PDFs, websites) to enable fast and accurate querying with LLMs.\n",
              "\n",
              "---\n",
              "\n",
              "**2. Core Use Cases**\n",
              "\n",
              "- **LangChain**:\n",
              "  - Building chatbots, virtual assistants, and complex multi-step workflows.  \n",
              "  - Managing conversational states and memory.  \n",
              "  - Orchestrating prompts and LLM calls with chains, agents, and tools.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Building semantic search engines over large documents.  \n",
              "  - Facilitating question-answering over custom data sources.  \n",
              "  - Creating indexes that efficiently retrieve relevant data for LLMs during inference.\n",
              "\n",
              "---\n",
              "\n",
              "**3. Architecture and Features**\n",
              "\n",
              "- **LangChain**:\n",
              "  - Offers a modular, flexible framework with components such as chains, agents, tools, memory, and prompt templates.  \n",
              "  - Integrates with multiple LLM providers (OpenAI, Cohere, AI21, etc.).  \n",
              "  - Supports complex workflows, prompt management, and conversation history.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Provides data ingestion, processing, and indexing tools (vector stores, SQL databases, etc.).  \n",
              "  - Offers various index types (e.g., tree, list, keyword, vector) for different retrieval strategies.  \n",
              "  - Easy integration with open-source LLMs and cloud providers.\n",
              "\n",
              "---\n",
              "\n",
              "**4. Integration and Extensibility**\n",
              "\n",
              "- **LangChain**:\n",
              "  - Designed to be highly extensible with custom components, tools, and integrations.  \n",
              "  - Widely adopted for building sophisticated LLM applications with diverse functionalities.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Focuses on data management and retrieval; integrates with vector databases, document loaders, and LLMs for QA systems.\n",
              "\n",
              "---\n",
              "\n",
              "**Summary**\n",
              "\n",
              "| Aspect | LangChain | LlamaIndex (GPT Index) |\n",
              "|---------|--------------|------------------------|\n",
              "| Main Purpose | Building conversational AI, workflows, agents | Indexing and querying large unstructured data for retrieval-augmented generation |\n",
              "| Focus | Orchestrating prompts, chains, and conversations | Data ingestion, indexing, retrieval |  \n",
              "| Use Cases | Chatbots, virtual assistants, multi-step workflows | Semantic search, Q&A over documents |  \n",
              "| Architecture | Modular, flexible chaining | Data indexing and retrieval mechanisms |  \n",
              "| Integration | Multiple LLM providers, tools | Data sources, vector stores, LLMs |\n",
              "\n",
              "---\n",
              "\n",
              "**In summary:**  \n",
              "**LangChain** is ideal if you're building complex conversational applications or workflows involving LLMs.  \n",
              "**LlamaIndex** is suitable when you want to create a system that can quickly retrieve relevant information from large datasets to answer questions or generate context-aware responses.\n",
              "\n",
              "---\n",
              "\n",
              "If you're choosing between them, consider your primary goal: conversational application vs. document retrieval and augmentation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't have personal preferences, but honestly, I'm so fed up with this pointless debate! Just give me my ice—crushed or cubed—and let me get on with it! I'm starving and just want some ice already!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I'm glad you're asking! If I could enjoy ice, I think I might prefer crushed ice because it's perfect for drinks where you'd want to quickly chill and get a refreshing, cold burst. Plus, it’s great for slushies or giving drinks a fun texture! But cubed ice is nice for more elegant beverages, like whiskey or cocktails served on the rocks. Both have their charm—depends on your mood! Which do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BzZBWQt8ml1Y6b8KeRKI1BaNWHViQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm glad you're asking! If I could enjoy ice, I think I might prefer crushed ice because it's perfect for drinks where you'd want to quickly chill and get a refreshing, cold burst. Plus, it’s great for slushies or giving drinks a fun texture! But cubed ice is nice for more elegant beverages, like whiskey or cocktails served on the rocks. Both have their charm—depends on your mood! Which do you prefer?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754012958, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=88, prompt_tokens=30, total_tokens=118, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in temperature, precipitation, and weather patterns primarily caused by human activities, such as burning fossil fuels, deforestation, and industrial processes. These actions increase greenhouse gases like carbon dioxide and methane in the atmosphere, leading to global warming. The impacts of climate change include rising sea levels, more frequent and severe extreme weather events, melting glaciers, and threats to biodiversity and agriculture. Addressing climate change requires global cooperation to reduce emissions, transition to renewable energy sources, and implement sustainable practices to protect the environment for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay nako, mga bakla at mga beshie! Ang climate change, ha, parang chisme na kumakalat sa barangay—laging nandiyan at palagi nangyayari! Pero seryoso, ha, dahil ito'y seryosong usapin na dapat nating pagtuunan ng pansin. Ang mundo natin, parang isang maleta na puno na, hindi na makahinga, kaya't kailangan nating maghugas ng kamay at magpakatino sa pagtatapon ng basura, pagtitipid sa enerhiya, at pagtutulungan para mapanatili ang ganda ng ating kalikasan. Recall, mga beshie, hindi lang ito usapin ng government, kundi usapin nating lahat! Kaya magkaisa tayo—para sa isang mas malamig, mas berde, at mas masayang planeta! Let's save Mother Earth, mga rakista!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench smoothly turned the falbean, ensuring everything was securely fastened."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The preview TTS version of Gemini 2.5 Pro is **not** considered a multimodal model.\n",
              "\n",
              "**Explanation:**\n",
              "\n",
              "- A **multimodal model** is one that can process and integrate multiple input formats (modalities), such as text, images, audio, or video, and often generate outputs across different modalities.\n",
              "\n",
              "- The **full Gemini 2.5 Pro** model processes multiple input formats—audio, images, text, and PDFs—and can generate text outputs, making it multimodal.\n",
              "\n",
              "- The **preview TTS version** only accepts **text input** and produces **audio output**. Since it processes only a single input modality (text), it is **not** multimodal. It is a specialized, modality-specific model designed for text-to-speech tasks.\n",
              "\n",
              "**Summary:**\n",
              "\n",
              "- **Is it multimodal?** **No**\n",
              "- **Reason:** Because it only handles one input modality (text) and does not process or combine multiple input types."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Multimodal models are generative models that can take multiple input formats such as text, audio, images, or video and can generate outputs in either a single format or a combination of formats. An example of a multimodal model is:\"),\n",
        "    assistant_prompt(\"GPT-4 is a multimodal model that accepts images and text and can generate text outputs from that combination.\"),\n",
        "    user_prompt(\"Gemini 2.5 Pro is capable of processing various input formats including audio, images, text, and PDF, and generates text outputs. The preview TTS version can only take text inputs and produce audio outputs. Is the preview TTS version of Gemini 2.5 Pro a multimodal model? If so, why? If not, why not?\")\n",
        "]\n",
        "\n",
        "multimodal_model_response = get_response(client, list_of_prompts)\n",
        "pretty_print(multimodal_model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 letter 'r's in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The word \"strawberry\" can be broken down by letters as follows:\n",
              "\n",
              "s - t - r - a - w - b - e - r - r - y\n",
              "\n",
              "The letter 'r' appears 3 times in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Breaking down into intermediate reasoning steps to enable complex reasoning for accurate results (Chain-of-Thought Prompting).\n",
        "reasoning_problem = \"\"\"\n",
        "Break down the word \"strawberry\" by letters. Count the number of occurences of the letter 'r'.\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "04-producton-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
