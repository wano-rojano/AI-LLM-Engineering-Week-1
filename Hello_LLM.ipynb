{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzP72Shu8JGK5rR0U3XLM7foLfTLw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both prominent frameworks designed to facilitate building applications that leverage large language models (LLMs), but they serve different purposes and have distinct focuses. Here's an overview of their main differences:\\n\\n**1. Purpose and Primary Focus:**\\n\\n- **LangChain:**  \\n  Focuses on building complex, multi-step language model applications, particularly those involving chains of prompts, agents, and orchestration. It provides tools for prompt management, memory, agents, and integrations with various data sources to create conversational AI, question-answering systems, and automation workflows.\\n\\n- **LlamaIndex (GPT Index):**  \\n  Centers on indexing and retrieving information from external data sources (like documents, PDFs, databases) to enable LLMs to perform knowledge-based tasks more effectively. Its main goal is to facilitate easy ingestion, structuring, and querying of large, unstructured data through optimized indices, making LLMs capable of answering questions based on specific datasets.\\n\\n**2. Core Functionality:**\\n\\n- **LangChain:**  \\n  - Building **language model pipelines** with chaining multiple prompts and logic.  \\n  - Managing **memory** and **state** across interactions.  \\n  - Creating **agents** that can decide which tools or modules to invoke dynamically.  \\n  - Integrating with various APIs, databases, and tools for complex applications.\\n\\n- **LlamaIndex:**  \\n  - Creating **indices** over external data sources (e.g., documents, spreadsheets).  \\n  - Supporting **querying** over these indices to retrieve relevant information.  \\n  - Providing tools for **data ingestion**, **preprocessing**, and **retrieval-augmented generation (RAG)** workflows.\\n\\n**3. Use Cases:**\\n\\n- **LangChain:**  \\n  - Building chatbots and conversational agents.  \\n  - Developing multi-step workflows (e.g., data extraction, summarization, reasoning).  \\n  - Automating processes that involve LLMs and various tools.\\n\\n- **LlamaIndex:**  \\n  - Building specialized Q&A systems over specific datasets.  \\n  - Organizing and indexing large collections of documents for efficient retrieval.  \\n  - Enabling retrieval-augmented generation by combining index lookup with LLM prompts.\\n\\n**4. Development Approach and Ecosystem:**\\n\\n- **LangChain:**  \\n  - Emphasizes a modular architecture with components like prompts, chains, agents, and memory.  \\n  - Supports multiple LLM providers and tools.  \\n  - Well-suited for complex, flexible applications requiring orchestrated interactions.\\n\\n- **LlamaIndex:**  \\n  - Focuses on data ingestion, indexing, and retrieval methods.  \\n  - Provides various index types (e.g., simple, tree, list, keyword, vector) to optimize performance.  \\n  - Facilitates easy construction of data-aware AI systems driven by external knowledge.\\n\\n---\\n\\n**Summary:**  \\nWhile **LangChain** is a comprehensive framework for orchestrating complex LLM workflows, including multi-turn conversations and tool integrations, **LlamaIndex** specializes in organizing and indexing external data to enable knowledge-aware querying with LLMs. Depending on your project requirements—whether you need complex conversational workflows or effective data retrieval—you might choose one over the other or even use them together.\\n\\n---\\n\\nIf you have a specific use case in mind, I can help guide which framework might be more suitable!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753974240, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=685, prompt_tokens=19, total_tokens=704, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Great question! LangChain and LlamaIndex are both popular frameworks used to facilitate building applications that leverage large language models (LLMs), but they have different focuses and use cases. Here's a breakdown of their main differences:\n",
              "\n",
              "### LangChain\n",
              "- **Purpose:** Primarily designed to enable developers to build complex, multi-step language model applications, including chains of prompts, chatbot workflows, and integrations with external tools.\n",
              "- **Key Features:**\n",
              "  - Modular components for prompt management, memory, and chaining.\n",
              "  - Support for various types of chains (sequential, mall, custom).\n",
              "  - Integration with different LLM providers (OpenAI, Hugging Face, etc.).\n",
              "  - Tooling for retrieval, lookups, and external API calls.\n",
              "  - Built-in abstractions for managing conversation state and context.\n",
              "- **Use Cases:** Chatbots, question-answering systems, automation workflows, and apps that need step-by-step processing or orchestration of LLM calls.\n",
              "- **Focus:** Orchestrating language models, managing context and state, and building complex prompts or workflows.\n",
              "\n",
              "### LlamaIndex (formerly GPT Index)\n",
              "- **Purpose:** Focused on enabling efficient and scalable retrieval-augmented generation (RAG) by indexing large external data sources, making LLMs capable of referencing and reasoning over large, structured, or unstructured datasets.\n",
              "- **Key Features:**\n",
              "  - Easy-to-use data ingestion pipelines for various data formats.\n",
              "  - Indexing and querying mechanisms to retrieve relevant data quickly.\n",
              "  - Tools to create, update, and query knowledge bases or document stores.\n",
              "  - Focus on integrating external data with LLMs for improved factual accuracy.\n",
              "- **Use Cases:** Building knowledge bases, document search, retrieval-augmented question answering, and managing large datasets for LLM applications.\n",
              "- **Focus:** Data ingestion, indexing, and retrieval over large datasets to enhance LLMs' factual and contextual abilities.\n",
              "\n",
              "---\n",
              "\n",
              "### Summary Table\n",
              "\n",
              "| Aspect                     | LangChain                                | LlamaIndex                                |\n",
              "|----------------------------|------------------------------------------|-------------------------------------------|\n",
              "| Primary Focus              | Workflow orchestration, chaining, prompts| Data indexing, retrieval-augmented generation (RAG) |\n",
              "| Use Cases                  | Chatbots, complex prompt workflows      | Knowledge bases, document retrieval     |\n",
              "| Data Handling              | Prompt management, memory, orchestration| Data ingestion, indexing, retrieval      |\n",
              "| Integration Scope          | LLM providers, tools, APIs              | Large datasets, external knowledge bases|\n",
              "| Approach                   | Modular chains and prompts               | Indexing large datasets for retrieval  |\n",
              "\n",
              "### In Summary:\n",
              "- **Choose LangChain** if you want to build complex chatbots, workflows, or multi-step LLM applications that require orchestration and prompt management.\n",
              "- **Choose LlamaIndex** if your goal is to manage large corpora of data, perform retrieval-augmented tasks, or create systems that need access to external knowledge sources efficiently.\n",
              "\n",
              "They can sometimes be used together in a complementary fashion—using LlamaIndex for data retrieval and LangChain for orchestrating the conversation or application logic.\n",
              "\n",
              "---\n",
              "\n",
              "Let me know if you'd like more detailed comparisons or examples!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? After waiting all this time to get some food, you're asking me about ice? Crushed ice is a disaster—melts way too fast, ruins the drink, and wastes perfectly good water! Cubed ice, on the other hand, at least stays cold longer and doesn't turn everything into a soupy mess! Honestly, I don't care what you prefer—I'm starving and all I want is some actual food, not ice trivia!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice has a fun, refreshing texture that's perfect for drinks like margaritas or cold desserts. Cubed ice stays colder longer and looks sleek in a glass. Both are great—depends on the vibe you're going for! Do you have a favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BzP7lwHUYCbxupObcnd7O6E8Z2hZd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice has a fun, refreshing texture that's perfect for drinks like margaritas or cold desserts. Cubed ice stays colder longer and looks sleek in a glass. Both are great—depends on the vibe you're going for! Do you have a favorite?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753974285, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=52, prompt_tokens=30, total_tokens=82, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in temperature, precipitation, and other atmospheric patterns primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes. These actions increase greenhouse gas concentrations in the atmosphere, leading to global warming. The effects of climate change include more frequent and severe weather events, rising sea levels, loss of biodiversity, and impacts on agriculture and human health. Addressing climate change requires global cooperation to reduce emissions, adopt sustainable practices, and transition to renewable energy sources."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello, everyone! It's me, Vice Ganda, and today, let's talk about something really important—climate change! Alam niyo ba, mga ka-idi, our planet is having a fever because of us. Umiinit! Paranginit din ang ulo ko kapag traffic, pero mas masakit yun, kasi nga, ang climate change, hindi lang joke-joke lang ‘to. It’s a serious issue that affects our environment, our health, at pati na rin ang mga kabataan natin in the future. Kaya mga ka-idi, magtulungan tayo! Reduce, reuse, recycle. Huwag kalimutan ang planeta, dahil siya ang bahay natin, di ba? Let's be responsible, para sa isang mas malamig at mas green na mundo. Muli, ito si Vice Ganda reminding: ‘Basta’t sama-sama tayo, kaya nating i-save ang earth! Mag-ingat, mga ka-idi, at always be kind to our planet!’"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench easily adjusts to fit the falbean, making the assembly process smooth and efficient."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are two \"r\"s in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The word \"strawberry\" can be broken down by letters as follows:\n",
              "\n",
              "s - t - r - a - w - b - e - r - r - y\n",
              "\n",
              "The letter 'r' appears 3 times in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Breaking down into intermediate reasoning steps to enable complex reasoning for accurate results (Chain-of-Thought Prompting).\n",
        "reasoning_problem = \"\"\"\n",
        "Break down the word \"strawberry\" by letters. Count the number of occurences of the letter 'r'.\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "04-producton-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
